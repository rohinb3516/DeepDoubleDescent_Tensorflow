{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N7NiICp5GYAO",
    "outputId": "64be8880-6e05-4758-f6f0-6c8cae20174f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fairseq\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/ab/92c6efb05ffdfe16fbdc9e463229d9af8c3b74dc943ed4b4857a87b223c2/fairseq-0.10.2-cp37-cp37m-manylinux1_x86_64.whl (1.7MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7MB 8.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq) (2019.12.20)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq) (4.41.1)\n",
      "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.14.5)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq) (0.29.22)\n",
      "Collecting hydra-core\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e3/fbd70dd0d3ce4d1d75c22d56c0c9f895cfa7ed6587a9ffb821d6812d6a60/hydra_core-1.0.6-py3-none-any.whl (123kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 33.4MB/s \n",
      "\u001b[?25hCollecting sacrebleu>=1.4.12\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 9.7MB/s \n",
      "\u001b[?25hCollecting dataclasses\n",
      "  Downloading https://files.pythonhosted.org/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.19.5)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.8.1+cu101)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq) (2.20)\n",
      "Collecting antlr4-python3-runtime==4.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 32.4MB/s \n",
      "\u001b[?25hCollecting omegaconf<2.1,>=2.0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq) (5.1.2)\n",
      "Collecting portalocker==2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->fairseq) (3.7.4.3)\n",
      "Collecting PyYAML>=5.1.*\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 30.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core->fairseq) (3.4.1)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp37-none-any.whl size=141231 sha256=a663d065a175d59c5c67599ca2fb6a63e9d5df5149975d6bf351ae94efecdf23\n",
      "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, PyYAML, omegaconf, hydra-core, portalocker, sacrebleu, dataclasses, fairseq\n",
      "  Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed PyYAML-5.4.1 antlr4-python3-runtime-4.8 dataclasses-0.6 fairseq-0.10.2 hydra-core-1.0.6 omegaconf-2.0.6 portalocker-2.0.0 sacrebleu-1.5.1\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pyarrow) (1.19.5)\n",
      "Cloning into 'fairseq'...\n",
      "remote: Enumerating objects: 27616, done.\u001b[K\n",
      "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
      "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
      "remote: Total 27616 (delta 15), reused 19 (delta 8), pack-reused 27579\u001b[K\n",
      "Receiving objects: 100% (27616/27616), 11.49 MiB | 21.99 MiB/s, done.\n",
      "Resolving deltas: 100% (20836/20836), done.\n",
      "Executed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "!git clone https://github.com/pytorch/fairseq\n",
    "!cd fairseq\n",
    "!pip install --editable ./\n",
    "\"\"\"\n",
    "!pip install fairseq\n",
    "\"\"\"\n",
    "!git clone https://github.com/NVIDIA/apex\n",
    "!cd apex\n",
    "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" --global-option=\"--deprecated_fused_adam\" --global-option=\"--xentropy\" --global-option=\"--fast_multihead_attn\" ./\n",
    "\"\"\"\n",
    "!pip install pyarrow\n",
    "!git clone https://github.com/pytorch/fairseq\n",
    "print(\"Executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YfASxahmNJK8",
    "outputId": "0d62ad5a-666f-4c56-c3b4-bb9b1d71a508"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "#os.chdir(\"..\")\n",
    "!rm -d -rf data-bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9mHbMFb5p19F",
    "outputId": "f6b88923-4c9b-49e4-e689-efc36e69e86b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 16 21:48:11 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   48C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x112kELyqmqc"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uY8Dmu7Mat8G",
    "outputId": "38221ca2-3c95-47a0-fda5-a517e492d8f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fairseq\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/ab/92c6efb05ffdfe16fbdc9e463229d9af8c3b74dc943ed4b4857a87b223c2/fairseq-0.10.2-cp37-cp37m-manylinux1_x86_64.whl (1.7MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7MB 18.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq) (2019.12.20)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq) (4.41.1)\n",
      "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.14.5)\n",
      "Collecting sacrebleu>=1.4.12\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 8.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq) (0.29.22)\n",
      "Collecting dataclasses\n",
      "  Downloading https://files.pythonhosted.org/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl\n",
      "Collecting hydra-core\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e3/fbd70dd0d3ce4d1d75c22d56c0c9f895cfa7ed6587a9ffb821d6812d6a60/hydra_core-1.0.6-py3-none-any.whl (123kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 55.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.19.5)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.8.1+cu101)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq) (2.20)\n",
      "Collecting portalocker==2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
      "Collecting antlr4-python3-runtime==4.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 49.3MB/s \n",
      "\u001b[?25hCollecting omegaconf<2.1,>=2.0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq) (5.1.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->fairseq) (3.7.4.3)\n",
      "Collecting PyYAML>=5.1.*\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 49.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core->fairseq) (3.4.1)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp37-none-any.whl size=141231 sha256=acdf2185cf5a51a50798bff9a52a6dd026d11a55afccac824852b1d74488dc07\n",
      "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: portalocker, sacrebleu, dataclasses, antlr4-python3-runtime, PyYAML, omegaconf, hydra-core, fairseq\n",
      "  Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed PyYAML-5.4.1 antlr4-python3-runtime-4.8 dataclasses-0.6 fairseq-0.10.2 hydra-core-1.0.6 omegaconf-2.0.6 portalocker-2.0.0 sacrebleu-1.5.1\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pyarrow) (1.19.5)\n",
      "Cloning into 'fairseq'...\n",
      "remote: Enumerating objects: 27621, done.\u001b[K\n",
      "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
      "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
      "remote: Total 27621 (delta 17), reused 22 (delta 10), pack-reused 27579\u001b[K\n",
      "Receiving objects: 100% (27621/27621), 11.47 MiB | 28.87 MiB/s, done.\n",
      "Resolving deltas: 100% (20832/20832), done.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0de-en/\n",
      "de-en/IWSLT14.TED.dev2010.de-en.de.xml\n",
      "de-en/IWSLT14.TED.dev2010.de-en.en.xml\n",
      "de-en/IWSLT14.TED.tst2010.de-en.de.xml\n",
      "de-en/IWSLT14.TED.tst2010.de-en.en.xml\n",
      "  1 19.0M    1  251k    0     0   410k      0  0:00:47 --:--:--  0:00:47  410kde-en/IWSLT14.TED.tst2011.de-en.de.xml\n",
      "de-en/IWSLT14.TED.tst2011.de-en.en.xml\n",
      "de-en/IWSLT14.TED.tst2012.de-en.de.xml\n",
      "de-en/IWSLT14.TED.tst2012.de-en.en.xml\n",
      "de-en/IWSLT14.TEDX.dev2012.de-en.de.xml\n",
      "de-en/IWSLT14.TEDX.dev2012.de-en.en.xml\n",
      "de-en/README\n",
      "de-en/train.en\n",
      "de-en/train.tags.de-en.de\n",
      "de-en/train.tags.de-en.en\n",
      "100 19.0M  100 19.0M    0     0  12.1M      0  0:00:01  0:00:01 --:--:-- 12.1M\n",
      "--2021-04-17 18:50:54--  http://www.statmt.org/europarl/v7/fr-en.tgz\n",
      "Resolving www.statmt.org (www.statmt.org)... 129.215.197.184\n",
      "Connecting to www.statmt.org (www.statmt.org)|129.215.197.184|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 202718517 (193M) [application/x-gzip]\n",
      "Saving to: ‘fr-en.tgz’\n",
      "\n",
      "fr-en.tgz           100%[===================>] 193.33M   107KB/s    in 32m 33s \n",
      "\n",
      "2021-04-17 19:23:27 (101 KB/s) - ‘fr-en.tgz’ saved [202718517/202718517]\n",
      "\n",
      "europarl-v7.fr-en.en\n",
      "europarl-v7.fr-en.fr\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9d58675db8c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mv europarl-v7.fr-en.en fr-en/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mv europarl-v7.fr-en.fr fr-en/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fr-en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;31m#!ls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Subsampling: 8:1:1 ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "!pip install fairseq\n",
    "!pip install pyarrow\n",
    "!git clone https://github.com/pytorch/fairseq\n",
    "import torch\n",
    "from fairseq import utils\n",
    "from fairseq.models import transformer, FairseqIncrementalDecoder, FairseqEncoder, FairseqEncoderDecoderModel, register_model, register_model_architecture\n",
    "\n",
    "#print(dir(transformer.TransformerEncoderLayer))\n",
    "\n",
    "embedding_dimension_model = [i for i in range(512)]\n",
    "dff = 4*embedding_dimension_model\n",
    "\n",
    "gradient_steps = 80000\n",
    "\n",
    "# Preprocess using BPE\n",
    "\n",
    "# 6layers, 8 attention heads per layer\n",
    "# Cross entropy loss is 10% label smoothed\n",
    "# No dropout\n",
    "# Batch size is 128\n",
    "\n",
    "\n",
    "# Datasets taken:\n",
    "# IWSLT'14s German to English de-en (160k)\n",
    "# WMT'14s English to French en-fr (subsampled to 200k) \n",
    "\n",
    "# Optimized using Adam, LR:1e-4\n",
    "# y(t) = y_o / sqrt(1+(t/512))\n",
    "\n",
    "# Have to implement early stopping as well, should not indicated DDD\n",
    "\n",
    "\n",
    "!curl http://dl.fbaipublicfiles.com/fairseq/data/iwslt14/de-en.tgz | tar xvzf -\n",
    "!wget http://www.statmt.org/europarl/v7/fr-en.tgz \n",
    "#os.chdir(\"..\")\n",
    "!tar -xvzf fr-en.tgz\n",
    "\n",
    "#!ls\n",
    "\n",
    "!mkdir fr-en\n",
    "!mv europarl-v7.fr-en.en fr-en/\n",
    "!mv europarl-v7.fr-en.fr fr-en/\n",
    "os.chdir(\"fr-en\")\n",
    "#!ls\n",
    "# Subsampling: 8:1:1 ratio\n",
    "!sed -n -e '1,200000p' europarl-v7.fr-en.en > train.en\n",
    "!sed -n -e '1,200000p' europarl-v7.fr-en.fr > train.fr\n",
    "\n",
    "!sed -n -e '200001,225000p' europarl-v7.fr-en.en > valid.en\n",
    "!sed -n -e '200001,225000p' europarl-v7.fr-en.fr > valid.fr\n",
    "\n",
    "!sed -n -e '225001,250000p' europarl-v7.fr-en.en > test.en\n",
    "!sed -n -e '225001,250000p' europarl-v7.fr-en.fr > test.fr\n",
    "\n",
    "print(\"Exceuted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyQIMEll-AVZ",
    "outputId": "ce5ce892-87bd-4319-c1a7-aa93b422a1fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/fr-en\n",
      "europarl-v7.fr-en.en  test.en  train.en  valid.en\n",
      "europarl-v7.fr-en.fr  test.fr  train.fr  valid.fr\n",
      "2021-04-17 00:33:47 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin/wmt14.tokenized.en-fr', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='en', srcdict=None, target_lang='fr', task='translation', tensorboard_logdir=None, testpref='test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='train', user_dir=None, validpref='valid', workers=1)\n",
      "2021-04-17 00:34:08 | INFO | fairseq_cli.preprocess | [en] Dictionary: 90136 types\n",
      "2021-04-17 00:34:29 | INFO | fairseq_cli.preprocess | [en] train.en: 200000 sents, 5238715 tokens, 0.0% replaced by <unk>\n",
      "2021-04-17 00:34:29 | INFO | fairseq_cli.preprocess | [en] Dictionary: 90136 types\n",
      "2021-04-17 00:34:31 | INFO | fairseq_cli.preprocess | [en] valid.en: 25000 sents, 666822 tokens, 0.926% replaced by <unk>\n",
      "2021-04-17 00:34:31 | INFO | fairseq_cli.preprocess | [en] Dictionary: 90136 types\n",
      "2021-04-17 00:34:34 | INFO | fairseq_cli.preprocess | [en] test.en: 25000 sents, 668243 tokens, 1.05% replaced by <unk>\n",
      "2021-04-17 00:34:34 | INFO | fairseq_cli.preprocess | [fr] Dictionary: 113504 types\n",
      "2021-04-17 00:34:56 | INFO | fairseq_cli.preprocess | [fr] train.fr: 200000 sents, 5497104 tokens, 0.0% replaced by <unk>\n",
      "2021-04-17 00:34:56 | INFO | fairseq_cli.preprocess | [fr] Dictionary: 113504 types\n",
      "2021-04-17 00:34:58 | INFO | fairseq_cli.preprocess | [fr] valid.fr: 25000 sents, 700759 tokens, 1.05% replaced by <unk>\n",
      "2021-04-17 00:34:58 | INFO | fairseq_cli.preprocess | [fr] Dictionary: 113504 types\n",
      "2021-04-17 00:35:01 | INFO | fairseq_cli.preprocess | [fr] test.fr: 25000 sents, 698244 tokens, 1.13% replaced by <unk>\n",
      "2021-04-17 00:35:01 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/wmt14.tokenized.en-fr\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls\n",
    "#os.chdir(\"de-en\")\n",
    "\n",
    "#!TOKEN_DE_EN=tokenized/iwslt14.tokenized.de-en\n",
    "!fairseq-preprocess --source-lang en --target-lang fr \\\n",
    "    --trainpref train \\\n",
    "    --validpref valid \\\n",
    "    --testpref test \\\n",
    "    --destdir data-bin/wmt14.tokenized.en-fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YTekjdkeQWAU",
    "outputId": "a19e78ca-8bc2-4bdd-f641-af525c614792"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-17 00:41:30 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_bos_token=False, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=128, batch_size_valid=128, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/wmt14.tokenized.en-fr', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.0, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, future_target=False, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0001], lr_scheduler='inverse_sqrt', max_epoch=80, max_target_positions=None, max_tokens=None, max_tokens_valid=None, max_update=50000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', output_dictionary_size=-1, past_target=False, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='none', save_dir='checkpoints/transformer_en_fr_summary', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, self_target=False, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_time_hours=0, task='language_modeling', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tokens_per_sample=128, tpu=False, train_subset='train.en-fr', update_freq=[10], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid.en-fr', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.01, zero_sharding='none')\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/train.py\", line 352, in cli_main\n",
      "    distributed_utils.call_main(args, main)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/distributed_utils.py\", line 301, in call_main\n",
      "    main(args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/train.py\", line 61, in main\n",
      "    task = tasks.setup_task(args)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/tasks/__init__.py\", line 28, in setup_task\n",
      "    return TASK_REGISTRY[task_cfg.task].setup_task(task_cfg, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/tasks/language_modeling.py\", line 158, in setup_task\n",
      "    dictionary, output_dictionary = cls.setup_dictionary(args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/tasks/language_modeling.py\", line 142, in setup_dictionary\n",
      "    dictionary = Dictionary.load(os.path.join(paths[0], \"dict.txt\"))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/data/dictionary.py\", line 214, in load\n",
      "    d.add_from_file(f)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/data/dictionary.py\", line 227, in add_from_file\n",
      "    raise fnfe\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/data/dictionary.py\", line 224, in add_from_file\n",
      "    with open(PathManager.get_local_path(f), \"r\", encoding=\"utf-8\") as fd:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data-bin/wmt14.tokenized.en-fr/dict.txt'\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 fairseq-train data-bin/wmt14.tokenized.en-fr \\\n",
    "--task language_modeling \\\n",
    "--train-subset train.en-fr \\\n",
    "--valid-subset valid.en-fr \\\n",
    "--save-interval=1 \\\n",
    "--save-dir checkpoints/transformer_en_fr_summary \\\n",
    "--arch transformer_iwslt_de_en \\\n",
    "--criterion=label_smoothed_cross_entropy --label-smoothing=0.1 \\\n",
    "--dropout 0 \\\n",
    "--encoder-embed-dim 512 --decoder-embed-dim 512 --encoder-ffn-embed-dim 2048 --decoder-ffn-embed-dim 2048 \\\n",
    "--encoder-layers 6 --encoder-attention-heads 8 \\\n",
    "--decoder-layers 6 --decoder-attention-heads 8 \\\n",
    "--optimizer adam  --weight-decay 0.01 \\\n",
    "--lr 0.0001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \\\n",
    "--batch-size 128 --tokens-per-sample 128 --sample-break-mode none --share-all-embeddings \\\n",
    "--update-freq 10 \\\n",
    "--fp16 \\\n",
    "--max-update 50000 \\\n",
    "--max-epoch 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1tkYRnxuvNem",
    "outputId": "3f5daefe-d7a8-4bc7-f480-2501e1036744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing commands...\n",
      "\n",
      "\n",
      "Downloaded IWSLT'14 dataset. Initializing preprocessing...\n",
      "\n",
      "\n",
      "Preprocessed en-fr. Moving to de-en...\n",
      "\n",
      "/content\n",
      "data-bin  de-en  fairseq  sample_data\n",
      "2021-04-16 23:15:22 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin/iwslt14.tokenized.de-en', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='en', srcdict=None, target_lang='de', task='translation', tensorboard_logdir=None, testpref='/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/train', user_dir=None, validpref='/valid', workers=1)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/fairseq-preprocess\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/preprocess.py\", line 394, in cli_main\n",
      "    main(args)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/preprocess.py\", line 103, in main\n",
      "    src_dict = build_dictionary([train_path(args.source_lang)], src=True)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/preprocess.py\", line 68, in build_dictionary\n",
      "    padding_factor=args.padding_factor,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/tasks/fairseq_task.py\", line 75, in build_dictionary\n",
      "    filename, d, tokenizer.tokenize_line, workers\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/data/dictionary.py\", line 365, in add_file_to_dictionary\n",
      "    filename, tokenize, dict.eos_word\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/data/dictionary.py\", line 324, in _add_file_to_dictionary_single_worker\n",
      "    with open(PathManager.get_local_path(filename), \"r\", encoding=\"utf-8\") as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/train.en'\n",
      "\n",
      "Preprocessed de-en. Initializing training of de-en...\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/train.py\", line 352, in cli_main\n",
      "    distributed_utils.call_main(args, main)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/distributed_utils.py\", line 301, in call_main\n",
      "    main(args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/train.py\", line 47, in main\n",
      "    ), \"Must specify batch size either with --max-tokens or --batch-size\"\n",
      "AssertionError: Must specify batch size either with --max-tokens or --batch-size\n",
      "\n",
      "Trained de-en. Initializing training of en-fr...\n",
      "\n",
      "\n",
      "Let's see how de-en model performs...\n",
      "\n",
      "usage: fairseq-eval-lm [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL]\n",
      "                       [--log-format {json,none,simple,tqdm}]\n",
      "                       [--tensorboard-logdir TENSORBOARD_LOGDIR] [--seed SEED]\n",
      "                       [--cpu] [--tpu] [--bf16] [--memory-efficient-bf16]\n",
      "                       [--fp16] [--memory-efficient-fp16]\n",
      "                       [--fp16-no-flatten-grads]\n",
      "                       [--fp16-init-scale FP16_INIT_SCALE]\n",
      "                       [--fp16-scale-window FP16_SCALE_WINDOW]\n",
      "                       [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\n",
      "                       [--min-loss-scale MIN_LOSS_SCALE]\n",
      "                       [--threshold-loss-scale THRESHOLD_LOSS_SCALE]\n",
      "                       [--user-dir USER_DIR]\n",
      "                       [--empty-cache-freq EMPTY_CACHE_FREQ]\n",
      "                       [--all-gather-list-size ALL_GATHER_LIST_SIZE]\n",
      "                       [--model-parallel-size MODEL_PARALLEL_SIZE]\n",
      "                       [--checkpoint-suffix CHECKPOINT_SUFFIX]\n",
      "                       [--checkpoint-shard-count CHECKPOINT_SHARD_COUNT]\n",
      "                       [--quantization-config-path QUANTIZATION_CONFIG_PATH]\n",
      "                       [--profile]\n",
      "                       [--criterion {label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,nat_loss,composite_loss,sentence_prediction,legacy_masked_lm_loss,wav2vec,sentence_ranking,ctc,adaptive_loss,masked_lm,cross_entropy,vocab_parallel_cross_entropy}]\n",
      "                       [--tokenizer {nltk,moses,space}]\n",
      "                       [--bpe {gpt2,sentencepiece,bytes,characters,byte_bpe,hf_byte_bpe,subword_nmt,fastbpe,bert}]\n",
      "                       [--optimizer {nag,adadelta,adagrad,adam,adafactor,adamax,lamb,sgd}]\n",
      "                       [--lr-scheduler {inverse_sqrt,polynomial_decay,fixed,reduce_lr_on_plateau,triangular,tri_stage,cosine}]\n",
      "                       [--scoring {sacrebleu,bleu,wer,chrf}] [--task TASK]\n",
      "                       [--num-workers NUM_WORKERS]\n",
      "                       [--skip-invalid-size-inputs-valid-test]\n",
      "                       [--max-tokens MAX_TOKENS] [--batch-size BATCH_SIZE]\n",
      "                       [--required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE]\n",
      "                       [--required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE]\n",
      "                       [--dataset-impl {raw,lazy,cached,mmap,fasta}]\n",
      "                       [--data-buffer-size DATA_BUFFER_SIZE]\n",
      "                       [--train-subset TRAIN_SUBSET]\n",
      "                       [--valid-subset VALID_SUBSET]\n",
      "                       [--validate-interval VALIDATE_INTERVAL]\n",
      "                       [--validate-interval-updates VALIDATE_INTERVAL_UPDATES]\n",
      "                       [--validate-after-updates VALIDATE_AFTER_UPDATES]\n",
      "                       [--fixed-validation-seed FIXED_VALIDATION_SEED]\n",
      "                       [--disable-validation]\n",
      "                       [--max-tokens-valid MAX_TOKENS_VALID]\n",
      "                       [--batch-size-valid BATCH_SIZE_VALID]\n",
      "                       [--curriculum CURRICULUM] [--gen-subset GEN_SUBSET]\n",
      "                       [--num-shards NUM_SHARDS] [--shard-id SHARD_ID]\n",
      "                       [--distributed-world-size DISTRIBUTED_WORLD_SIZE]\n",
      "                       [--distributed-rank DISTRIBUTED_RANK]\n",
      "                       [--distributed-backend DISTRIBUTED_BACKEND]\n",
      "                       [--distributed-init-method DISTRIBUTED_INIT_METHOD]\n",
      "                       [--distributed-port DISTRIBUTED_PORT]\n",
      "                       [--device-id DEVICE_ID] [--distributed-no-spawn]\n",
      "                       [--ddp-backend {c10d,no_c10d}]\n",
      "                       [--bucket-cap-mb BUCKET_CAP_MB] [--fix-batches-to-gpus]\n",
      "                       [--find-unused-parameters] [--fast-stat-sync]\n",
      "                       [--broadcast-buffers]\n",
      "                       [--distributed-wrapper {DDP,SlowMo}]\n",
      "                       [--slowmo-momentum SLOWMO_MOMENTUM]\n",
      "                       [--slowmo-algorithm SLOWMO_ALGORITHM]\n",
      "                       [--localsgd-frequency LOCALSGD_FREQUENCY]\n",
      "                       [--nprocs-per-node NPROCS_PER_NODE]\n",
      "                       [--pipeline-model-parallel]\n",
      "                       [--pipeline-balance PIPELINE_BALANCE]\n",
      "                       [--pipeline-devices PIPELINE_DEVICES]\n",
      "                       [--pipeline-chunks PIPELINE_CHUNKS]\n",
      "                       [--pipeline-encoder-balance PIPELINE_ENCODER_BALANCE]\n",
      "                       [--pipeline-encoder-devices PIPELINE_ENCODER_DEVICES]\n",
      "                       [--pipeline-decoder-balance PIPELINE_DECODER_BALANCE]\n",
      "                       [--pipeline-decoder-devices PIPELINE_DECODER_DEVICES]\n",
      "                       [--pipeline-checkpoint {always,never,except_last}]\n",
      "                       [--zero-sharding {none,os}] [--path PATH]\n",
      "                       [--remove-bpe [REMOVE_BPE]] [--quiet]\n",
      "                       [--model-overrides MODEL_OVERRIDES]\n",
      "                       [--results-path RESULTS_PATH] [--output-word-probs]\n",
      "                       [--output-word-stats] [--context-window CONTEXT_WINDOW]\n",
      "                       [--softmax-batch SOFTMAX_BATCH] [--force-anneal N]\n",
      "                       [--lr-shrink LS] [--warmup-updates N]\n",
      "                       [--sample-break-mode {none,complete,complete_doc,eos}]\n",
      "                       [--tokens-per-sample TOKENS_PER_SAMPLE]\n",
      "                       [--output-dictionary-size OUTPUT_DICTIONARY_SIZE]\n",
      "                       [--self-target] [--future-target] [--past-target]\n",
      "                       [--add-bos-token]\n",
      "                       [--max-target-positions MAX_TARGET_POSITIONS]\n",
      "                       [--shorten-method {none,truncate,random_crop}]\n",
      "                       [--shorten-data-split-list SHORTEN_DATA_SPLIT_LIST]\n",
      "                       data\n",
      "fairseq-eval-lm: error: unrecognized arguments: --max-sentences 2\n",
      "\n",
      "Complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Commands:\n",
    "print(\"\\nInitializing commands...\\n\")\n",
    "\n",
    "#!git clone https://github.com/pytorch/fairseq\n",
    "\n",
    "# Downloading data & preprocessing\n",
    "\n",
    "#!curl https://dl.fbaipublicfiles.com/fairseq/models/wmt14.v2.en-fr.fconv-py.tar.bz2 | tar xvjf -\n",
    "\n",
    "#print(\"\\nDownloaded WMT'14 dataset. Preparing the rest...\\n\")\n",
    "\"\"\"\n",
    "os.chdir(\"fairseq/examples/translation/\")\n",
    "!pwd\n",
    "!bash prepare-iwslt14.sh\n",
    "#!bash prepare-wmt14en2fr.sh\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "\"\"\"\n",
    "print(\"\\nDownloaded IWSLT'14 dataset. Initializing preprocessing...\\n\")\n",
    "\"\"\"\n",
    "!TOKEN_EN_FR=examples/translation/wmt14_en_fr\n",
    "!fairseq-preprocess --source-lang en --target-lang fr \\\n",
    "    --trainpref $TOKEN_EN_FR/train --validpref $TOKEN_EN_FR/valid --testpref $TOKEN_EN_FR/test \\\n",
    "    --destdir data-bin/wmt14_en_fr\n",
    "\"\"\"\n",
    "print(f\"\\nPreprocessed en-fr. Moving to de-en...\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nPreprocessed de-en. Initializing training of de-en...\\n\")\n",
    "\n",
    "\"\"\"\n",
    "!cd fairseq/\n",
    "!DATASET=/path/to/dataset\n",
    "!fairseq-preprocess --only-source --source-lang=de --target-lang=en --lr-shrink 0.5 --trainpref $DATASET/train.txt --validpref $DATASET/valid.txt --testpref $DATASET/test.txt --destdir data-bin/summary --workers 20\n",
    "\"\"\"\n",
    "# --task language_modeling --max-tokens 2048 --activation-fn relu --warmup-updates 4000 --warmup-init-lr 1e-07\n",
    "# Train\n",
    "!CUDA_VISIBLE_DEVICES=0 fairseq-train data-bin/iwslt14.tokenized.de-en \\\n",
    "--task language_modeling \\\n",
    "--save-interval=1 \\\n",
    "--save-dir checkpoints/transformer_de_en_summary \\\n",
    "--arch transformer_iwslt_de_en \\\n",
    "--criterion=label_smoothed_cross_entropy --label-smoothing=0.1 \\\n",
    "--dropout 0 \\\n",
    "--encoder-embed-dim 512 --decoder-embed-dim 512 --encoder-ffn-embed-dim 2048 --decoder-ffn-embed-dim 2048 \\\n",
    "--encoder-layers 6 --encoder-attention-heads 8 \\\n",
    "--decoder-layers 6 --decoder-attention-heads 8 \\\n",
    "--optimizer adam  --weight-decay 0.01 \\\n",
    "--lr 0.0001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \\\n",
    "--tokens-per-sample 128 --sample-break-mode none --share-all-embeddings \\\n",
    "--update-freq 10 \\\n",
    "--fp16 \\\n",
    "--max-update 50000 \\\n",
    "--max-epoch 80\n",
    "\n",
    "print(\"\\nTrained de-en. Initializing training of en-fr...\\n\")\n",
    "\"\"\"\n",
    "!CUDA_VISIBLE_DEVICES=0 fairseq-train data-bin/wmt14_en_fr \\\n",
    "--task language_modeling \\\n",
    "--save-interval=1 \\\n",
    "--save-dir checkpoints/transformer_en_fr_summary \\\n",
    "--arch  transformer_vaswani_wmt_en_fr_big \\\n",
    "--criterion=label_smoothed_cross_entropy --label-smoothing=0.1\\\n",
    "--dropout 0 \\\n",
    "--encoder-embed-dim 512 --decoder-embed-dim 512 --encoder-ffn-embed-dim 2048 --decoder-ffn-embed-dim 2048 \\\n",
    "--encoder-layers 6 --encoder-attention-heads 8 \\\n",
    "--decoder-layers 6 --decoder-attention-heads 8 \\\n",
    "--optimizer adam  --weight-decay 0.01 \\\n",
    "--lr 0.0001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \\\n",
    "--tokens-per-sample 128 --sample-break-mode none --share-all-embeddings \\\n",
    "--update-freq 10 \\\n",
    "--fp16 \\\n",
    "--max-update 50000 \\\n",
    "--max-epoch 80\n",
    "\n",
    "print(\"\\nTrained en-fr. Let's see how en-fr model performs...\\n\")\n",
    "\n",
    "# Evaluate language model\n",
    "\n",
    "!fairseq-eval-lm data-bin/wmt14_en_fr \\\n",
    "--path checkpoints/transformer_en_fr_summary/checkpoint_best.pt \\\n",
    "--max-sentences 2 \\\n",
    "--tokens-per-sample 128 \\\n",
    "--context-window 400\n",
    "\"\"\"\n",
    "print(\"\\nLet's see how de-en model performs...\\n\")\n",
    "\n",
    "!fairseq-eval-lm data-bin/iwslt14.tokenized.de-en \\\n",
    "--path checkpoints/transformer_de_en_summary/checkpoint_best.pt \\\n",
    "--max-sentences 2 \\\n",
    "--tokens-per-sample 128 \\\n",
    "--context-window 400\n",
    "\n",
    "# Take loss calculated for each model trained and plot the required graphs\n",
    "\n",
    "print(\"\\nComplete.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pr3jW63p7joW"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bBmVo8zmJyw",
    "outputId": "9a44d4bd-c11d-4cd8-f1dd-4a715d0537c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/fairseq\n",
      "code  test.de  test.en\ttmp  train.de  train.en  valid.de  valid.en\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "#!ls\n",
    "\"\"\"\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "\"\"\"\n",
    "#os.chdir(\"examples/translation/\")\n",
    "!ls examples/translation/iwslt14.tokenized.de-en\n",
    "#!pip install fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-bBRfwCPgWSi",
    "outputId": "2a58a129-c8c7-40e9-9b29-cc03d3c624de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing commands...\n",
      "\n",
      "\n",
      "Downloaded WMT'14 dataset. Preparing the rest...\n",
      "\n",
      "\n",
      "Downloaded IWSLT'14 dataset. Initializing preprocessing...\n",
      "\n",
      "/content/fairseq\n",
      "build\t\t    data-bin  fairseq_cli\tpyproject.toml\ttests\n",
      "checkpoints\t    docs      fairseq.egg-info\tREADME.md\ttrain.py\n",
      "CODE_OF_CONDUCT.md  examples  hubconf.py\tscripts\n",
      "CONTRIBUTING.md     fairseq   LICENSE\t\tsetup.py\n",
      "2021-04-14 14:30:22 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin/iwslt14.tokenized.de-en', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang=None, srcdict=None, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, testpref='examples/translation/iwslt14.tokenized.de-en/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='examples/translation/iwslt14.tokenized.de-en/train', use_plasma_view=False, user_dir=None, validpref='examples/translation/iwslt14.tokenized.de-en/valid', wandb_project=None, workers=1)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/fairseq-preprocess\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-preprocess')())\n",
      "  File \"/content/fairseq/fairseq_cli/preprocess.py\", line 394, in cli_main\n",
      "    main(args)\n",
      "  File \"/content/fairseq/fairseq_cli/preprocess.py\", line 103, in main\n",
      "    src_dict = build_dictionary([train_path(args.source_lang)], src=True)\n",
      "  File \"/content/fairseq/fairseq_cli/preprocess.py\", line 68, in build_dictionary\n",
      "    padding_factor=args.padding_factor,\n",
      "  File \"/content/fairseq/fairseq/tasks/fairseq_task.py\", line 119, in build_dictionary\n",
      "    filename, d, tokenizer.tokenize_line, workers\n",
      "  File \"/content/fairseq/fairseq/data/dictionary.py\", line 372, in add_file_to_dictionary\n",
      "    filename, tokenize, dict.eos_word\n",
      "  File \"/content/fairseq/fairseq/data/dictionary.py\", line 325, in _add_file_to_dictionary_single_worker\n",
      "    with open(PathManager.get_local_path(filename), \"r\", encoding=\"utf-8\") as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'examples/translation/iwslt14.tokenized.de-en/train'\n",
      "\n",
      "Preprocessed de-en. Initializing training of de-en...\n",
      "\n",
      "2021-04-14 14:30:26 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'distributed_num_procs': 0}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 128, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 128, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 80, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [10], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/transformer_de_en_summary', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_bos_token=False, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.tokenized.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.0, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, future_target=False, gen_subset='test', heartbeat_timeout=-1, ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0001], lr_scheduler='inverse_sqrt', max_epoch=80, max_target_positions=None, max_tokens=128, max_tokens_valid=128, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', output_dictionary_size=-1, pad=1, pad_to_fixed_bsz=False, pad_to_fixed_length=False, past_target=False, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='none', save_dir='checkpoints/transformer_de_en_summary', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, self_target=False, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, task='language_modeling', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tokens_per_sample=128, tpu=False, train_subset='train', unk=3, update_freq=[10], use_bmuf=False, use_old_adam=False, use_plasma_view=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'language_modeling', 'data': 'data-bin/iwslt14.tokenized.de-en', 'sample_break_mode': 'none', 'tokens_per_sample': 128, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': 128, 'batch_size_valid': 128, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/fairseq-train\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\n",
      "  File \"/content/fairseq/fairseq_cli/train.py\", line 491, in cli_main\n",
      "    distributed_utils.call_main(cfg, main)\n",
      "  File \"/content/fairseq/fairseq/distributed/utils.py\", line 369, in call_main\n",
      "    main(cfg, **kwargs)\n",
      "  File \"/content/fairseq/fairseq_cli/train.py\", line 82, in main\n",
      "    task = tasks.setup_task(cfg.task)\n",
      "  File \"/content/fairseq/fairseq/tasks/__init__.py\", line 44, in setup_task\n",
      "    return task.setup_task(cfg, **kwargs)\n",
      "  File \"/content/fairseq/fairseq/tasks/language_modeling.py\", line 169, in setup_task\n",
      "    dictionary, output_dictionary = cls.setup_dictionary(args, **kwargs)\n",
      "  File \"/content/fairseq/fairseq/tasks/language_modeling.py\", line 153, in setup_dictionary\n",
      "    dictionary = Dictionary.load(os.path.join(paths[0], \"dict.txt\"))\n",
      "  File \"/content/fairseq/fairseq/data/dictionary.py\", line 215, in load\n",
      "    d.add_from_file(f)\n",
      "  File \"/content/fairseq/fairseq/data/dictionary.py\", line 228, in add_from_file\n",
      "    raise fnfe\n",
      "  File \"/content/fairseq/fairseq/data/dictionary.py\", line 225, in add_from_file\n",
      "    with open(PathManager.get_local_path(f), \"r\", encoding=\"utf-8\") as fd:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data-bin/iwslt14.tokenized.de-en/dict.txt'\n",
      "\n",
      "Let's see how de-en model performs...\n",
      "\n",
      "2021-04-14 14:30:31 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/transformer_de_en_summary/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'distributed_num_procs': 0}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 2, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 2, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 400, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/iwslt14.tokenized.de-en', 'sample_break_mode': 'none', 'tokens_per_sample': 128, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': 2, 'batch_size_valid': 2, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/fairseq-eval-lm\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-eval-lm')())\n",
      "  File \"/content/fairseq/fairseq_cli/eval_lm.py\", line 343, in cli_main\n",
      "    distributed_utils.call_main(convert_namespace_to_omegaconf(args), main)\n",
      "  File \"/content/fairseq/fairseq/distributed/utils.py\", line 369, in call_main\n",
      "    main(cfg, **kwargs)\n",
      "  File \"/content/fairseq/fairseq_cli/eval_lm.py\", line 249, in main\n",
      "    task = tasks.setup_task(cfg.task)\n",
      "  File \"/content/fairseq/fairseq/tasks/__init__.py\", line 44, in setup_task\n",
      "    return task.setup_task(cfg, **kwargs)\n",
      "  File \"/content/fairseq/fairseq/tasks/language_modeling.py\", line 169, in setup_task\n",
      "    dictionary, output_dictionary = cls.setup_dictionary(args, **kwargs)\n",
      "  File \"/content/fairseq/fairseq/tasks/language_modeling.py\", line 153, in setup_dictionary\n",
      "    dictionary = Dictionary.load(os.path.join(paths[0], \"dict.txt\"))\n",
      "  File \"/content/fairseq/fairseq/data/dictionary.py\", line 215, in load\n",
      "    d.add_from_file(f)\n",
      "  File \"/content/fairseq/fairseq/data/dictionary.py\", line 228, in add_from_file\n",
      "    raise fnfe\n",
      "  File \"/content/fairseq/fairseq/data/dictionary.py\", line 225, in add_from_file\n",
      "    with open(PathManager.get_local_path(f), \"r\", encoding=\"utf-8\") as fd:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data-bin/iwslt14.tokenized.de-en/dict.txt'\n",
      "\n",
      "Complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Commands:\n",
    "print(\"\\nInitializing commands...\\n\")\n",
    "\n",
    "#!git clone https://github.com/pytorch/fairseq\n",
    "\n",
    "# Downloading data & preprocessing\n",
    "\n",
    "#!curl https://dl.fbaipublicfiles.com/fairseq/models/wmt14.v2.en-fr.fconv-py.tar.bz2 | tar xvjf -\n",
    "\n",
    "print(\"\\nDownloaded WMT'14 dataset. Preparing the rest...\\n\")\n",
    "\n",
    "\"\"\"\n",
    "os.chdir(\"examples/translation/\")\n",
    "!pwd\n",
    "!bash prepare-iwslt14.sh\n",
    "#!bash prepare-wmt14en2fr.sh\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "\"\"\"\n",
    "print(\"\\nDownloaded IWSLT'14 dataset. Initializing preprocessing...\\n\")\n",
    "\"\"\"\n",
    "!TOKEN_EN_FR=examples/translation/wmt14_en_fr\n",
    "!fairseq-preprocess --source-lang en --target-lang fr \\\n",
    "    --trainpref $TOKEN_EN_FR/train --validpref $TOKEN_EN_FR/valid --testpref $TOKEN_EN_FR/test \\\n",
    "    --destdir data-bin/wmt14_en_fr\n",
    "\n",
    "print(\"\\nPreprocessed en-fr. Moving to de-en...\\n\")\n",
    "\"\"\"\n",
    "#!ls examples/translation/\n",
    "#os.chdir(\"examples/translation/iwslt14.tokenized.de-en\")\n",
    "\"\"\"\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "\"\"\"\n",
    "#os.chdir(\"..\")\n",
    "\n",
    "!pwd\n",
    "!ls\n",
    "#!TOKEN_DE_EN= examples/translation/iwslt14.tokenized.de-en\n",
    "!fairseq-preprocess --source-lang de --target-lang en \\\n",
    "    --trainpref examples/translation/iwslt14.tokenized.de-en/train \\\n",
    "    --validpref examples/translation/iwslt14.tokenized.de-en/valid \\\n",
    "    --testpref examples/translation/iwslt14.tokenized.de-en/test \\\n",
    "    --destdir data-bin/iwslt14.tokenized.de-en\n",
    "\n",
    "print(\"\\nPreprocessed de-en. Initializing training of de-en...\\n\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "!cd fairseq/\n",
    "!DATASET=/path/to/dataset\n",
    "!fairseq-preprocess --only-source --trainpref $DATASET/train.txt --validpref $DATASET/valid.txt --testpref $DATASET/test.txt --destdir data-bin/summary --workers 20\n",
    "\"\"\"\n",
    "#!ls data-bin/iwslt14.tokenized.de-en\n",
    "\n",
    "# --task language_modeling --max-tokens 2048 --activation-fn relu --warmup-updates 4000 --warmup-init-lr 1e-07\n",
    "# Train\n",
    "!CUDA_VISIBLE_DEVICES=0 fairseq-train data-bin/iwslt14.tokenized.de-en \\\n",
    "--task language_modeling \\\n",
    "--save-interval=1 \\\n",
    "--save-dir checkpoints/transformer_de_en_summary \\\n",
    "--arch transformer_iwslt_de_en \\\n",
    "--criterion=label_smoothed_cross_entropy --label-smoothing=0.1 \\\n",
    "--dropout 0 \\\n",
    "--encoder-embed-dim 512 --decoder-embed-dim 512 --encoder-ffn-embed-dim 2048 --decoder-ffn-embed-dim 2048 \\\n",
    "--encoder-layers 6 --encoder-attention-heads 8 \\\n",
    "--decoder-layers 6 --decoder-attention-heads 8 \\\n",
    "--optimizer adam  --weight-decay 0.01 \\\n",
    "--lr 0.0001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \\\n",
    "--tokens-per-sample 128 --sample-break-mode none --share-all-embeddings \\\n",
    "--update-freq 10 \\\n",
    "--fp16 \\\n",
    "--max-update 50000 \\\n",
    "--batch-size 128 --max-tokens 128 \\\n",
    "--max-epoch 80\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nTrained de-en. Initializing training of en-fr...\\n\")\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0 fairseq-train data-bin/wmt14_en_fr \\\n",
    "--task language_modeling \\\n",
    "--save-interval=1 \\\n",
    "--save-dir checkpoints/transformer_en_fr_summary \\\n",
    "--arch  transformer_vaswani_wmt_en_fr_big \\\n",
    "--criterion=label_smoothed_cross_entropy --label-smoothing=0.1\\\n",
    "--source-lang=en --target-lang=fr \\\n",
    "--dropout 0 \\\n",
    "--encoder-embed-dim 512 --decoder-embed-dim 512 --encoder-ffn-embed-dim 2048 --decoder-ffn-embed-dim 2048 \\\n",
    "--encoder-layers 6 --encoder-attention-heads 8 \\\n",
    "--decoder-layers 6 --decoder-attention-heads 8 \\\n",
    "--optimizer adam  --weight-decay 0.01 \\\n",
    "--lr 0.0001 --lr-scheduler inverse_sqrt --lr-shrink 0.5 --warmup-updates 4000 --warmup-init-lr 1e-07 \\\n",
    "--tokens-per-sample 128 --sample-break-mode none --share-all-embeddings \\\n",
    "--update-freq 10 \\\n",
    "--fp16 \\\n",
    "--max-update 50000 \\\n",
    "--max-epoch 80\n",
    "\n",
    "print(\"\\nTrained en-fr. Let's see how en-fr model performs...\\n\")\n",
    "\"\"\"\n",
    "# Evaluate language model\n",
    "\"\"\"\n",
    "!fairseq-eval-lm data-bin/wmt14_en_fr \\\n",
    "--path checkpoints/transformer_en_fr_summary/checkpoint_best.pt \\\n",
    "--max-sentences 2 \\\n",
    "--tokens-per-sample 128 \\\n",
    "--context-window 400\n",
    "\"\"\"\n",
    "print(\"\\nLet's see how de-en model performs...\\n\")\n",
    "\n",
    "!fairseq-eval-lm data-bin/iwslt14.tokenized.de-en \\\n",
    "--path checkpoints/transformer_de_en_summary/checkpoint_best.pt \\\n",
    "--max-sentences 2 \\\n",
    "--tokens-per-sample 128 \\\n",
    "--context-window 400\n",
    "\n",
    "# Take loss calculated for each model trained and plot the required graphs\n",
    "\n",
    "print(\"\\nComplete.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3I87LZXxikrs"
   },
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "\n",
    "!cd fairseq/\n",
    "!DATASET=/path/to/dataset\n",
    "!fairseq-preprocess --only-source --trainpref $DATASET/train.txt --validpref $DATASET/valid.txt --testpref $DATASET/test.txt --destdir data-bin/summary --workers 20\n",
    "\n",
    "print(\"Executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "FGpq_ie_uhZI"
   },
   "outputs": [],
   "source": [
    "# Multihead attention\n",
    "# Could be used directly from the fairseq library\n",
    "\n",
    "from fairseq.modules.multihead_attention import MultiheadAttention\n",
    "import torch.nn as nn\n",
    "\n",
    "#@with_incremental_state\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "      pass\n",
    "    # Applies Xavier parameter initialization\n",
    "    def reset_parameters(self):\n",
    "      pass\n",
    "    # See discussion below\n",
    "    def forward(\n",
    "        self,\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        key_padding_mask,\n",
    "        incremental_state,\n",
    "        need_weights,\n",
    "        static_kv,\n",
    "        attn_mask,\n",
    "        before_softmax,\n",
    "        need_head_weights,\n",
    "    ):\n",
    "      pass\n",
    "      #  -> Tuple[Tensor, Optional[Tensor]]\n",
    "\n",
    "    # concatnate key_padding_mask from current time step to previous\n",
    "    # time step. Required for incremental decoding.\n",
    "    @staticmethod\n",
    "    def _append_prev_key_padding_mask():\n",
    "      pass\n",
    "      #  -> Optional[Tensor]\n",
    "\n",
    "    # reorder incremental state according to new_order vector\n",
    "    # Not used??\n",
    "    def reorder_incremental_state():\n",
    "      pass\n",
    "    # _input_buffer includes states from a previous time step.\n",
    "    # saved to 'attn_state' in its incremental state\n",
    "    def _get_input_buffer():\n",
    "      pass\n",
    "      #  -> Dict[str, Optional[Tensor]]\n",
    "      \n",
    "    def _set_input_buffer():\n",
    "      pass\n",
    "    # Empty hook for internal use\n",
    "    def apply_sparse_mask(attn_weights, tgt_len: int, src_len: int, bsz: int):\n",
    "      pass\n",
    "    def upgrade_state_dict_named(self, state_dict, name):\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBum1rl9o-oG"
   },
   "outputs": [],
   "source": [
    "# Create extended Transformer Encoder class \n",
    "\n",
    "\"\"\"\n",
    "EncoderOut = NamedTuple(\n",
    "    \"EncoderOut\",\n",
    "    [\n",
    "        (\"encoder_out\", Tensor),  # T x B x C\n",
    "        (\"encoder_padding_mask\", Tensor),  # B x T\n",
    "        (\"encoder_embedding\", Tensor),  # B x T x C\n",
    "        (\"encoder_states\", Optional[List[Tensor]]),  # List[T x B x C]\n",
    "    ],\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "class TransformerEncoder(FairseqEncoder):\n",
    "  # initialize all layers, modeuls needed in forward\n",
    "  # including TransformerEncoderlayer, LayerNorm,\n",
    "  # PositionalEmbedding etc.\n",
    "  # embed_tokens is an `Embedding` instance, which\n",
    "  # defines how to embed a token (word2vec, GloVE etc.)\n",
    "  def __init__(self, args, dictionary, embed_tokens):...\n",
    "\n",
    "  # forward embedding takes the raw token and pass through\n",
    "  # embedding layer, positional enbedding, layer norm and\n",
    "  # dropout\n",
    "  def forward_embedding(self, src_tokens):...\n",
    "\n",
    "  # Forward pass of a transformer encoder. Chains of\n",
    "  # TransformerEncoderLayer. Returns EncoderOut type.\n",
    "  def forward(\n",
    "      self,\n",
    "      src_tokens,\n",
    "      src_lengths,\n",
    "      cls_input: Optional[Tensor] = None,\n",
    "      return_all_hiddens: bool = False,\n",
    "  ):...\n",
    "\n",
    "  def reorder_encoder_out(self, encoder_out: EncoderOut, new_order):...\n",
    "  def max_positions(self):...\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, args):...\n",
    "    def upgrade_state_dict_named(self, state_dict, name):...\n",
    "    def forward(self, x, encoder_padding_mask, attn_mask: Optional[Tensor] = None):..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfJ3nVbgpPuX"
   },
   "outputs": [],
   "source": [
    "# Create extended Transformer Decoder class\n",
    "# Use incremental decoder for speed (to be tested)\n",
    "\n",
    "class TransformerDecoder(FairseqIncrementalDecoder):\n",
    "    # Similar to TransformerEncoder::__init__\n",
    "    def __init__(self, args, dictionary, embed_tokens, no_encoder_attn=False):...\n",
    "    # Wraps over extract_features()\n",
    "    def forward(...):...\n",
    "    # Applies feed forward functions to encoder output. See below discussion\n",
    "    def extract_features(\n",
    "        prev_output_tokens,\n",
    "        encoder_out,\n",
    "        incremental_state,\n",
    "        full_context_alignment,\n",
    "        alignment_layer,\n",
    "        alignment_heads,\n",
    "    ):...\n",
    "    # Convert from feature size to vocab size.\n",
    "    def output_layer(self, features):...\n",
    "    def max_positions(self):...\n",
    "    # Retrieves if mask for future tokens is buffered in the class\n",
    "    def buffered_future_mask(self, tensor):...\n",
    "    def upgrade_state_dict_named(self, state_dict, name):...\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    # setup components required for forward\n",
    "    def __init__(\n",
    "        self, args, no_encoder_attn=False, add_bias_kv=False, add_zero_attn=False\n",
    "    ):...\n",
    "    # Requres when running the model on onnx backend.\n",
    "    def prepare_for_onnx_export_(self):...\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        encoder_out,\n",
    "        encoder_padding_mask,\n",
    "        incremental_state,\n",
    "        prev_self_attn_state,\n",
    "        prev_attn_state,\n",
    "        self_attn_mask,\n",
    "        self_attn_padding_mask,\n",
    "        need_attn,\n",
    "        need_head_weights,\n",
    "    ):..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POdlA7RijZoy"
   },
   "outputs": [],
   "source": [
    "# Create extended transformer model class\n",
    "\n",
    "import torch\n",
    "from fairseq.models import register_model,FairseqEncoderDecoderModel\n",
    "\n",
    "@register_model(\"upgraded_transformer\")\n",
    "class UpgradedTransformerModel(FairseqEncoderDecoderModel):\n",
    "    \n",
    "    # defines where to retrive pretrained model from torch hub\n",
    "    @classmethod\n",
    "    def hub_models(cls):...\n",
    "    \n",
    "    # pass in arguments from command line, initialize encoder and decoder\n",
    "    def __init__(self, args, encoder, decoder):...\n",
    "    \n",
    "    # adds argument to command line entrance\n",
    "    @classmethod\n",
    "    def add_args(parser):...\n",
    "    \n",
    "    # compute encoding for input, construct encoder and decoder, returns a\n",
    "    # Transformer instance\n",
    "    @classmethod\n",
    "    def bulid_model(cls, args, task):...\n",
    "    \n",
    "    # helper function to build an encoder\n",
    "    @classmethod\n",
    "    def build_encoder(cls, args, src_dict, embed_tokens):...\n",
    "    \n",
    "    # helper function to build a decoder\n",
    "    @classmethod\n",
    "    def build_decoder(cls, args, tgt_dict, embed_tokens):...\n",
    "\n",
    "    # mostly the same with FairseqEncoderDecoderModel::forward, connects\n",
    "    # encoder and decoder.\n",
    "    def forward(\n",
    "      self, src_tokens, src_lengths, prv_output_tokens,\n",
    "      cls_input, return_all_hiddens, features_only,\n",
    "      alingment_layer, alignement_heads\n",
    "      ):..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zht_iTW6jdJt"
   },
   "outputs": [],
   "source": [
    "# Register the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opFNx28Ojl5R"
   },
   "outputs": [],
   "source": [
    "# Register model architecture\n",
    "\n",
    "@register_model_architecture(\"transformer\", \"transformer\")\n",
    "def base_architecture(args):...\n",
    "\n",
    "@register_model_architecture(\"transformer\", \"transformer_iwslt_de_en\")\n",
    "def transformer_iwslt_de_en(args):...\n",
    "\n",
    "@register_model_architecture(\"transformer\", \"transformer_wmt_en_de\")\n",
    "def transformer_wmt_en_de(args):...\n",
    "\n",
    "# parameters used in the \"Attention Is All You Need\" paper (Vaswani et al., 2017)\n",
    "@register_model_architecture(\"transformer\", \"transformer_vaswani_wmt_en_de_big\")\n",
    "def transformer_vaswani_wmt_en_de_big(args):...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ir8A8AI-jpkH"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling \\\n",
    "data-bin/summary \\\n",
    "--save-dir checkpoints/transformer_summary \\\n",
    "--arch transformer_lm --share-decoder-input-output-embed \\\n",
    "--dropout 0.1 \\\n",
    "--optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0 \\\n",
    "--lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \\\n",
    "--tokens-per-sample 512 --sample-break-mode none \\\n",
    "--max-tokens 2048 --update-freq 16 \\\n",
    "--fp16 \\\n",
    "--max-update 50000 \\\n",
    "--max-epoch 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Svpz2nOukOLI"
   },
   "outputs": [],
   "source": [
    "# Evaluate language model\n",
    "\n",
    "!fairseq-eval-lm data-bin/summary \\\n",
    "--path checkpoints/transformer_summary/checkpoint_best.pt \\\n",
    "--max-sentences 2 \\\n",
    "--tokens-per-sample 512 \\\n",
    "--context-window 400\n",
    "\n",
    "# Take loss calculated for each model trained and plot the required graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLmTITNzkVPi"
   },
   "outputs": [],
   "source": [
    "# To generate text (& check validity of model)\n",
    "\n",
    "# Beam search\n",
    "!fairseq-interactive data-bin/summary \\\n",
    "--task language_modeling \\\n",
    "--path checkpoints/transformer_summary/checkpoint_best.pt \\\n",
    "--beam 5\n",
    "\n",
    "# Top-k sampling\n",
    "!fairseq-interactive data-bin/summary \\\n",
    "--task language_modeling \\\n",
    "--path checkpoints/transformer_summary/checkpoint_best.pt \\\n",
    "--sampling --beam 1 --sampling-topk 10\n",
    "\n",
    "# Top-p sampling\n",
    "!fairseq-interactive data-bin/summary \\\n",
    "--task language_modeling \\\n",
    "--path checkpoints/transformer_summary/checkpoint_best.pt \\\n",
    "--sampling --beam 1 --sampling-topp 0.8\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Fairseq_test.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
