{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9B-g-UOS3n3",
    "outputId": "2c34eb15-e0ba-4f9f-a70e-be16ab6ed951"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DeepDoubleDescent_Tensorflow'...\n",
      "remote: Enumerating objects: 561, done.\u001b[K\n",
      "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
      "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
      "remote: Total 561 (delta 8), reused 18 (delta 8), pack-reused 541\u001b[K\n",
      "Receiving objects: 100% (561/561), 309.99 MiB | 28.17 MiB/s, done.\n",
      "Resolving deltas: 100% (262/262), done.\n",
      "Checking out files: 100% (459/459), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/AlwaysSearching/DeepDoubleDescent_Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_piEJIj9XZts",
    "outputId": "035a22f8-a0f2-40ae-b276-ad8cba9b0b2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 18 04:44:54 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   44C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yT3eQCirXia1",
    "outputId": "c16aa696-a567-42f3-fb4f-1d5f9a2a609d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "import os\n",
    "os.chdir(\"DeepDoubleDescent_Tensorflow\")\n",
    "\n",
    "from utils.train_utils import train_resnet18\n",
    "from utils.visualizations import plot_loss_from_file, plot_loss_vs_epoch_from_file, load_results\n",
    "\n",
    "# keeps tensorflow from using all available GPU memory when a model is initialized.\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"Executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctYke-NrXy8a",
    "outputId": "5bbaac08-2f34-41da-8f69-dfec0dfc5592"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CovNet on CIFAR-100 with 0 Label Noise for 500 epochs - Recreating figure 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Block for generating Figure 7 plot\n",
    "\n",
    "########### IMPORTANT ####################\n",
    "# This is crashing the colab notebook\n",
    "# Even lost the previous work due to this \n",
    "# Only try running this before saving previous work\n",
    "# Did not try running this in GCP\n",
    "\n",
    "from utils.train_utils import train_conv_nets\n",
    "data_set = 'cifar100'\n",
    "#Assigned: 13-16, 20, 48-56\n",
    "covnet_widths_fig7 = [i for i in range(17)]+[20+(4*i) for i in range(12)]\n",
    "\n",
    "print(\"\\nTraining CovNet on CIFAR-100 with 0 Label Noise for 500 epochs - Recreating figure 7\\n\")\n",
    "\n",
    "label_noise = 0\n",
    "\"\"\"\n",
    "metrics_noise0_rohin = list()\n",
    "metrics_noise0_rohin = train_conv_nets(\n",
    "    data_set=data_set,\n",
    "    resnet_widths=resnet_widths_rohin,\n",
    "    label_noise_as_int=label_noise,\n",
    "    n_epochs=1000\n",
    ")\"\"\"\n",
    "\n",
    "metrics_fig7 = train_conv_nets(\n",
    "    data_set=data_set,\n",
    "    convnet_depth=5,\n",
    "    convnet_widths=covnet_widths_fig7,\n",
    "    label_noise_as_int=0,\n",
    "    n_batch_steps=200_000,\n",
    "    optimizer=None,\n",
    "    save=True,\n",
    "    data_save_path_prefix=\"\",\n",
    "    data_save_path_suffix=\"\",\n",
    "    load_saved_metrics=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ahYDPhgTTpD",
    "outputId": "3a42b3b3-0402-416c-937f-1dc20a68e4af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed\n"
     ]
    }
   ],
   "source": [
    "# Updated train_resnet18 to run for a list amount of epochs\n",
    "###################################\n",
    "# WRONG APPROACH for Fig 16-18\n",
    "# Keeping code for future reference\n",
    "###################################\n",
    "\n",
    "######## IMPORTANT ########\n",
    "\n",
    "# No need to update load_results() in utils/visualizations.py\n",
    "# But would need to update plot_loss_from_file(), implementation is giving errors\n",
    "# Use plot_loss_from_file() to visualise results vs epochs\n",
    "# Also, just to visualize progress better, would need to update timer() callback class\n",
    "\n",
    "# This means use epoch as a list instead of width in plot_loss_vs_epoch_from_file()\n",
    "# The axis labels will be wrong so might want to change that\n",
    "# Probably would wanna remove the transpose in train_error.T in line 231,\n",
    "# which is- train_im = train_plot.imshow(train_error.T, aspect=\"auto\", origin=\"lower\", norm=norm, interpolation=\"nearest\")\n",
    "\n",
    "from utils.train_utils import load_data, timer\n",
    "from models.resnet import make_resnet18_UniformHe\n",
    "\n",
    "def train_resnet18_epoch(\n",
    "    data_set,\n",
    "    resnet_widths,\n",
    "    label_noise_as_int=10,\n",
    "    n_epochs=None,\n",
    "    n_batch_steps=500_000,\n",
    "    optimizer=None,\n",
    "    save=True,\n",
    "    data_save_path_prefix=\"\",\n",
    "    data_save_path_suffix=\"\",\n",
    "    load_saved_metrics=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and save the results of ResNets nets of a given range of model widths.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_set: str\n",
    "        Which data set to train on. See the load data funciton.\n",
    "    resnet_widths: int\n",
    "        Model width to train.\n",
    "    label_noise_as_int: int\n",
    "        Percentage of label noise to add to the training data.\n",
    "    n_epochs: list[int]\n",
    "        number of epochs to train as a list, if not specified, will calculate with n_batch_steps\n",
    "    n_batch_steps: int\n",
    "        number of gradient descent steps to take, over-ridden if n_epochs is specified\n",
    "    optimizer: tf.keras.optimizer\n",
    "        Optimizer to use while training resnets. Default is Adam with a learning rate of 1e-4.\n",
    "    save: bool\n",
    "        whether to save the data and trained model weights.\n",
    "    data_save_path_prefix: str\n",
    "        prefix to add to the save pkl file path.\n",
    "    data_save_path_suffix: str\n",
    "        suffix to add to the save pkl file name.\n",
    "    load_saved_metrics: bool\n",
    "        if True, will attempt to load the metrics from a previous training session in the save_path,\n",
    "        to continue training from there. If True, will load the saved .pkl file instead of starting\n",
    "        over and overwriting it. \n",
    "    \"\"\"\n",
    "\n",
    "    label_noise = label_noise_as_int / 100\n",
    "\n",
    "    # load the relevent dataset\n",
    "    (x_train, y_train), (x_test, y_test), image_shape = load_data(\n",
    "        data_set, label_noise, augment_data=False\n",
    "    )\n",
    "\n",
    "    batch_size = 128\n",
    "    n_classes = tf.math.reduce_max(y_train).numpy() + 1\n",
    "\n",
    "    \"\"\"\n",
    "    # total number desirec SGD steps / number batches per epoch = n_epochs\n",
    "    if not n_epochs:\n",
    "        n_epochs = n_batch_steps // (x_train.shape[0] // batch_size)\n",
    "\t\"\"\"\n",
    "\n",
    "    # store results for later graphing and analysis.\n",
    "    model_histories = {}\n",
    "    metrics = {}\n",
    "\n",
    "    # Paths to save model weights and experimental results.\n",
    "    model_weights_paths = f\"trained_model_weights_epoch_{data_set}/resnet18_{label_noise_as_int}pct_noise_width_{resnet_widths}/\"\n",
    "    data_save_path = (\n",
    "        f\"experimental_results_epoch_{data_set}/resnet18_{label_noise_as_int}pct_noise_width_{resnet_widths}\" + \".pkl\"\n",
    "    )\n",
    "\n",
    "    # add possible path identifiers.\n",
    "    if data_save_path_prefix:\n",
    "        data_save_path = data_save_path_prefix + \"/\" + data_save_path\n",
    "    if data_save_path_suffix:\n",
    "        assert data_save_path[-4:] == \".pkl\"\n",
    "        data_save_path = data_save_path[:-4] + data_save_path_suffix + \".pkl\"\n",
    "    \n",
    "    # load data from prior runs of related experiment.\n",
    "    if load_saved_metrics:\n",
    "        try:\n",
    "            with open(data_save_path, 'rb') as f:\n",
    "                metrics = pkl.load(f)\n",
    "        except Exception as e:\n",
    "            print('Could not find saved metrics.pkl file, exiting')\n",
    "            raise e\n",
    "\n",
    "        loaded_epochs = [int(i.split('_')[-1]) for i in metrics.keys()]\n",
    "        assert n_epochs[:len(loaded_epochs)] == loaded_epochs\n",
    "        print('loaded results for epoch %s from existing file at %s' %(', '.join([str(i) for i in loaded_epochs]), data_save_path))\n",
    "\n",
    "        assert data_save_path[-4:] == \".pkl\"\n",
    "        data_backup_path = data_save_path[:-4] + 'backup_w%d_' %loaded_epochs[-1] + time.strftime(\"%D_%H%M%S\").replace('/', '') + \".pkl\"\n",
    "        print('saving existing result.pkl to backup at %s' %data_backup_path)\n",
    "        pkl.dump(metrics, open(data_backup_path, \"wb\"))\n",
    "\n",
    "    # Resnet18 with Kaiming Uniform Initialization.\n",
    "    resnet, model_id = make_resnet18_UniformHe(\n",
    "        image_shape, k=resnet_widths, num_classes=n_classes\n",
    "    )\n",
    "\n",
    "    # compile and pass input to initialize parameters.\n",
    "    resnet.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-4)\n",
    "        if optimizer is None\n",
    "        else optimizer,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    resnet(tf.keras.Input(shape=list(image_shape), batch_size=batch_size))\n",
    "\n",
    "    m_id = model_id\n",
    "\n",
    "    for epoch in n_epochs:\n",
    "        if load_saved_metrics and epoch in loaded_epochs:\n",
    "            print('epoch %d results already loaded from .pkl file, training skipped' %epoch)\n",
    "            continue\n",
    "        \n",
    "        # Different widths were recorded in model_id via make_resnet18_UniformHe().\n",
    "        # Epochs would need to be mentioned in model_id to make sure the dict is accessible by number of epochs, is taken care of here\n",
    "        # Make sure this is done when writing train_covnet_epoch()\n",
    "\n",
    "        model_id = f\"{m_id}_epochs_{epoch}\"\n",
    "\n",
    "        model_timer = timer()\n",
    "\n",
    "        print(f\"STARTING TRAINING: {model_id}, Label Noise: {label_noise}\")\n",
    "        history = resnet.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            validation_data=(x_test, y_test),\n",
    "            epochs=epoch,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            callbacks=[model_timer],\n",
    "        )\n",
    "        print(f\"FINISHED TRAINING: {model_id}\")\n",
    "\n",
    "        # add results to dictionary and store the resulting model weights.\n",
    "        metrics[model_id] = history.history\n",
    "\n",
    "        # clear GPU of prior model to decrease training times.\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        # Save results to the data file\n",
    "        if save:\n",
    "            pkl.dump(metrics, open(data_save_path, \"wb\"))\n",
    "            history.model.save_weights(model_weights_paths + model_id)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"Executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OZmz__sKKne5",
    "outputId": "06aac80e-27be-44db-f306-a2f6725ff04f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3, 5, 6, 7, 10, 12, 15, 19, 25, 158, 31, 39, 50, 316, 63, 199, 79, 100, 251, 125}\n",
      "1516\n",
      "Hours to generate graph:  12.633333333333333\n"
     ]
    }
   ],
   "source": [
    "# Calculations for generating list of epochs (need to minimise total number to yield a result in time)\n",
    "# Logic: If \"no. of epochs\" axis is logarithmic in nature, no need to plot high number of epochs on the logarithmic unit scale\n",
    "#        Visualize how many points we need to achieve a plot resembling the curve, choose that as n below\n",
    "###################################\n",
    "# WRONG APPROACH for Fig 16-18\n",
    "# Keeping code for future reference\n",
    "###################################\n",
    "\n",
    "# n is number of points we need to plot to generate the graph\n",
    "n = 10\n",
    "\n",
    "# Assume the x-axis needs ~300 epochs\n",
    "# Needs to be precise till ~200 epochs for a good plot\n",
    "# x-axis is logarithmic\n",
    "\n",
    "#epoch_fig16 = [(i+1)*4 for i in range(50)] +[200+(8*i) for i in range(12)] # Epochs: 8028\n",
    "\n",
    "gap = 10**(1/n)\n",
    "\n",
    "epoch_fig16 = [ int(gap**(i+1)) for i in range(25)]\n",
    "\n",
    "epoch_fig16 = set(epoch_fig16)\n",
    "print(epoch_fig16)\n",
    "#print(set(epoch_fig16))\n",
    "# Total number of epochs\n",
    "print(sum(epoch_fig16))\n",
    "# Each epoch takes half a minute, 25 epochs ~12 minutes\n",
    "print(\"Hours to generate graph: \",(sum(epoch_fig16)*0.5)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "OZY_KtVXpABi",
    "outputId": "f96ce1c5-4a78-476a-a230-20d051ef500e"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-8a3b28e22e09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mlabel_noise_as_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_noise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_fig16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-27-5557f40700cb>\u001b[0m in \u001b[0;36mtrain_resnet18_epoch\u001b[0;34m(data_set, resnet_widths, label_noise_as_int, n_epochs, n_batch_steps, optimizer, save, data_save_path_prefix, data_save_path_suffix, load_saved_metrics)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_timer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         )\n\u001b[1;32m    147\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"FINISHED TRAINING: {model_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test for train_resnet18_epoch()\n",
    "###################################\n",
    "# WRONG APPROACH for Fig 16-18\n",
    "# Keeping code for future reference\n",
    "###################################\n",
    "# Ran this code for epochs 4-36 in steps of 4.\n",
    "# Result stored in path \"experimental_results_cifar10/resnet18_0pct_noise_width_32_wrong.pkl\"\n",
    "\n",
    "import utils\n",
    "#print(dir(utils.train_utils))\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "\n",
    "import importlib\n",
    "importlib.import_module(\"utils.train_utils\")\n",
    "\n",
    "#from utils.train_utils import train_resnet18_epoch\n",
    "data_set = 'cifar10'\n",
    "#Assigned: 13-16, 20, 48-56\n",
    "#epoch_fig16 = [i+1 for i in range(200)]+[200+(4*i) for i in range(25)]\n",
    "\n",
    "\"\"\"\n",
    "# Estimated time for running: 44 hrs\n",
    "epoch_fig16 = [(i+1)*4 for i in range(50)] +[200+(8*i) for i in range(12)] # Current estimate ~8028 epochs\n",
    "\"\"\"\n",
    "\n",
    "n=10\n",
    "\n",
    "gap = 10**(1/n)\n",
    "\n",
    "epoch_fig16 = [ int(gap**(i+1)) for i in range(25)]\n",
    "\n",
    "epoch_fig16 = set(epoch_fig16)\n",
    "\n",
    "print(\"\\nTraining ResNet on CIFAR-10 with 20 Label Noise with width 32 for 1-300 epochs - Recreating figure 16\\n\")\n",
    "# Doing this without data augmentation\n",
    "\n",
    "label_noise = 0\n",
    "  \n",
    "metrics_noise0_rohin = list()\n",
    "metrics_noise0_rohin = train_resnet18_epoch(\n",
    "  data_set=data_set,\n",
    "  resnet_widths=32,\n",
    "  label_noise_as_int=label_noise,\n",
    "  n_epochs=epoch_fig16,\n",
    "  optimizer=tf.keras.optimizers.Adam(1e-4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHtB76HvZ7gi",
    "outputId": "721a4068-3a53-4453-e5b4-2af6d48f951a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'loss': [0.6852884292602539, 0.6756851077079773, 0.5962361693382263, 0.6272801160812378, 0.5792248845100403, 0.44380801916122437, 0.4084590673446655, 0.3449031114578247, 0.7192893028259277, 0.3113483488559723, 0.2922412157058716, 0.3441198468208313], 'accuracy': [0.7637400031089783, 0.7697799801826477, 0.793179988861084, 0.7965999841690063, 0.8085799813270569, 0.8486400246620178, 0.8596600294113159, 0.8805000185966492, 0.8135799765586853, 0.8949800133705139, 0.904699981212616, 0.8934800028800964], 'val_loss': [5.388895034790039, 2.888650894165039, 1.2670987844467163, 2.710080862045288, 2.406886577606201, 3.8777894973754883, 2.7102694511413574, 1.3832056522369385, 1.9802799224853516, 3.9091312885284424, 2.6442079544067383, 3.713196039199829], 'val_accuracy': [0.3813000023365021, 0.4472000002861023, 0.6320000290870667, 0.44769999384880066, 0.5260999798774719, 0.3659000098705292, 0.5314000248908997, 0.6680999994277954, 0.6406000256538391, 0.4575999975204468, 0.5440999865531921, 0.4291999936103821]}\n",
      "dict_items([('loss', [0.6852884292602539, 0.6756851077079773, 0.5962361693382263, 0.6272801160812378, 0.5792248845100403, 0.44380801916122437, 0.4084590673446655, 0.3449031114578247, 0.7192893028259277, 0.3113483488559723, 0.2922412157058716, 0.3441198468208313]), ('accuracy', [0.7637400031089783, 0.7697799801826477, 0.793179988861084, 0.7965999841690063, 0.8085799813270569, 0.8486400246620178, 0.8596600294113159, 0.8805000185966492, 0.8135799765586853, 0.8949800133705139, 0.904699981212616, 0.8934800028800964]), ('val_loss', [5.388895034790039, 2.888650894165039, 1.2670987844467163, 2.710080862045288, 2.406886577606201, 3.8777894973754883, 2.7102694511413574, 1.3832056522369385, 1.9802799224853516, 3.9091312885284424, 2.6442079544067383, 3.713196039199829]), ('val_accuracy', [0.3813000023365021, 0.4472000002861023, 0.6320000290870667, 0.44769999384880066, 0.5260999798774719, 0.3659000098705292, 0.5314000248908997, 0.6680999994277954, 0.6406000256538391, 0.4575999975204468, 0.5440999865531921, 0.4291999936103821])])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Ran this code for epochs 4-36 in steps of 4.\n",
    "# Result stored in path \"experimental_results_cifar10/resnet18_0pct_noise_width_32_wrong.pkl\"\n",
    "###################################\n",
    "# WRONG APPROACH for Fig 16-18\n",
    "# Keeping code for future reference\n",
    "###################################\n",
    "\n",
    "with open(\"experimental_results_epoch_cifar10/resnet18_0pct_noise_width_32.pkl\", 'rb') as f:\n",
    "  metrics_loaded = pkl.load(f)\n",
    "print(len(metrics_loaded.items()))\n",
    "print(metrics_loaded['ResNet18_width_32'])\n",
    "print(metrics_loaded['ResNet18_width_32'].items())\n",
    "print(len(metrics_loaded['ResNet18_width_32'].items()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Fig7_16-18_epochwise_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
